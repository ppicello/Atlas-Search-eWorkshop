{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "SjlZsXW_hi5_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f11ab5afbd974449979ca7d2213c59ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Your Atlas URI:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_64c0c37f384f4709ae97f557ff6dee9f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c77757b45fb54627b82384902716d0dd",
            "value": ""
          }
        },
        "64c0c37f384f4709ae97f557ff6dee9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c77757b45fb54627b82384902716d0dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "125px"
          }
        },
        "7c4aa69ba3034d688bcc9d43a88c5cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Your OpenAI API key:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_94b0103422884d0886d65e1c738e81da",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1e0176f79ea14d95aeef6417b7dcb249",
            "value": ""
          }
        },
        "94b0103422884d0886d65e1c738e81da": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e0176f79ea14d95aeef6417b7dcb249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "125px"
          }
        },
        "4d0d48457ed34ea986886f133a70b40b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "TextModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "TextView",
            "continuous_update": true,
            "description": "Your username:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_4219f28a667748ee9ef2f63e2d3c60bd",
            "placeholder": "username",
            "style": "IPY_MODEL_1ec5332d8b5a4c0a8c5a6858bc321da9",
            "value": ""
          }
        },
        "4219f28a667748ee9ef2f63e2d3c60bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ec5332d8b5a4c0a8c5a6858bc321da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "125px"
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ppicello/Atlas-Search-eWorkshop/blob/main/MDB_RAG_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MongoDB Vector Search Workshop - RETRIEVAL AUGMENTED GENERATION (RAG)\n",
        "\n",
        "In this notebook we will build a RAG architecture to ask questions about PDF documents uploaded in a folder within your Google Drive account. We will leverage [MongoDB Atlas](https://www.mongodb.com/products/platform/atlas-vector-search) as vector store, [OpenAI](https://openai.com/) as embedding model and LLM and [Langchain](https://www.langchain.com/) as LLM framework. The choice of using Google Drive was just to make it easier for participants to upload documents.\n",
        "\n",
        "\n",
        "> There are no requirements in order to run this notebook, you should be able to run it completely in Google Colab\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1QWp1TpFQmFcv9lcmkAtpARTrzyBBUDHY)\n"
      ],
      "metadata": {
        "id": "fnMEK_5edZfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Google Colab instruction\n",
        "\n",
        "A notebook is a list of cells. Cells contain either explanatory text or executable code and its output. Click a cell to select it.\n",
        "\n",
        "Below is a **code cell**. Click in the cell to select it and execute the contents in the following ways:\n",
        "\n",
        "* Click the **Play icon** in the left gutter of the cell;\n",
        "* Type **Shift+Enter** to run the cell and move focus to the next cell (adding one if none exists)"
      ],
      "metadata": {
        "id": "w5ksDl9W6aDr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = 10\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrrQH8-A6iCx",
        "outputId": "a7162023-5871-42c2-e708-1d1e5f7e4a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 0 - Install dependencies\n",
        "In this step we will install the dependencies needed (like pymongo, lancgain etc). This can take a couple of minutes.\n"
      ],
      "metadata": {
        "id": "SjlZsXW_hi5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python -m pip install pymongo pypdf langchain openai tiktoken gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BXpvxc5bh7cI",
        "outputId": "10a21098-e7fe-4daa-9ce4-8abc4bff7155"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo\n",
            "  Downloading pymongo-4.6.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m677.2/677.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf\n",
            "  Downloading pypdf-4.0.2-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain\n",
            "  Downloading langchain-0.1.9-py3-none-any.whl (816 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m817.0/817.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio\n",
            "  Downloading gradio-4.19.2-py3-none-any.whl (16.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dnspython<3.0.0,>=1.16.0 (from pymongo)\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.21 (from langchain)\n",
            "  Downloading langchain_community-0.0.24-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.26 (from langchain)\n",
            "  Downloading langchain_core-0.1.27-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m250.8/250.8 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain)\n",
            "  Downloading langsmith-0.1.9-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Collecting fastapi (from gradio)\n",
            "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.10.1 (from gradio)\n",
            "  Downloading gradio_client-0.10.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m307.9/307.9 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.3)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.3)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Collecting orjson~=3.0 (from gradio)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart>=0.0.9 (from gradio)\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting ruff>=0.2.2 (from gradio)\n",
            "  Downloading ruff-0.2.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting tomlkit==0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.10.1->gradio) (2023.6.0)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==0.10.1->gradio)\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.16.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Collecting colorama<0.5.0,>=0.4.3 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting shellingham<2.0.0,>=1.3.0 (from typer[all]<1.0,>=0.9->gradio)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi->gradio)\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.33.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Building wheels for collected packages: ffmpy\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=a651b400f0a28681d94ae0de3725bd15e6fe4eb428a5840008009c79fa659e77\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built ffmpy\n",
            "Installing collected packages: pydub, ffmpy, websockets, tomlkit, shellingham, semantic-version, ruff, python-multipart, pypdf, orjson, mypy-extensions, marshmallow, jsonpointer, h11, dnspython, colorama, aiofiles, uvicorn, typing-inspect, tiktoken, starlette, pymongo, jsonpatch, httpcore, langsmith, httpx, fastapi, dataclasses-json, openai, langchain-core, gradio-client, langchain-community, gradio, langchain\n",
            "Successfully installed aiofiles-23.2.1 colorama-0.4.6 dataclasses-json-0.6.4 dnspython-2.6.1 fastapi-0.110.0 ffmpy-0.3.2 gradio-4.19.2 gradio-client-0.10.1 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.9 langchain-community-0.0.24 langchain-core-0.1.27 langsmith-0.1.9 marshmallow-3.21.0 mypy-extensions-1.0.0 openai-1.12.0 orjson-3.9.15 pydub-0.25.1 pymongo-4.6.2 pypdf-4.0.2 python-multipart-0.0.9 ruff-0.2.2 semantic-version-2.10.0 shellingham-1.5.4 starlette-0.36.3 tiktoken-0.6.0 tomlkit-0.12.0 typing-inspect-0.9.0 uvicorn-0.27.1 websockets-11.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1 - Setup MongoDB and OpenAI\n",
        "\n",
        "*example:*\n",
        "```\n",
        "mongodb_uri='mongodb+srv://<username>:<password>@cluster0.4m8aa.mongodb.net/?retryWrites=true&w=majority'\n",
        "openai_apikey='sk-XXXXXXXXXXXXXX'\n",
        "username = 'picellopaolo'\n",
        "```\n",
        "\n",
        "Feel free to use your own Atlas cluster and OpenAI apikey if you have one.\n",
        "\n",
        "**Remember to choose your username**, it will be used to isolate your vectors from other participants when using the same Atlas cluster (please use only lowercase letters, ex: `jamessmith`)"
      ],
      "metadata": {
        "id": "SEo3rMNyenTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "import os\n",
        "\n",
        "mongodb_uri_widget = widgets.Password(\n",
        "    description='Your Atlas URI:',\n",
        "    disabled=False,\n",
        "    style=dict(description_width='125px')\n",
        ")\n",
        "\n",
        "openai_api_key_widget = widgets.Password(\n",
        "    description='Your OpenAI API key:',\n",
        "    disabled=False,\n",
        "    style=dict(description_width='125px')\n",
        ")\n",
        "\n",
        "username_widget = widgets.Text(\n",
        "    value='',\n",
        "    placeholder='username',\n",
        "    description='Your username:',\n",
        "    style=dict(description_width='125px'),\n",
        "    disabled=False\n",
        ")\n",
        "\n",
        "\n",
        "display(mongodb_uri_widget)\n",
        "display(openai_api_key_widget)\n",
        "display(username_widget)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109,
          "referenced_widgets": [
            "f11ab5afbd974449979ca7d2213c59ab",
            "64c0c37f384f4709ae97f557ff6dee9f",
            "c77757b45fb54627b82384902716d0dd",
            "7c4aa69ba3034d688bcc9d43a88c5cbb",
            "94b0103422884d0886d65e1c738e81da",
            "1e0176f79ea14d95aeef6417b7dcb249",
            "4d0d48457ed34ea986886f133a70b40b",
            "4219f28a667748ee9ef2f63e2d3c60bd",
            "1ec5332d8b5a4c0a8c5a6858bc321da9"
          ]
        },
        "id": "RXVD68jQfP_-",
        "outputId": "178febf4-2e78-40b6-c0ab-611b3ad97d82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Password(description='Your Atlas URI:', style=DescriptionStyle(description_width='125px'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f11ab5afbd974449979ca7d2213c59ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Password(description='Your OpenAI API key:', style=DescriptionStyle(description_width='125px'))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c4aa69ba3034d688bcc9d43a88c5cbb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Text(value='', description='Your username:', placeholder='username', style=DescriptionStyle(description_width=‚Ä¶"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d0d48457ed34ea986886f133a70b40b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cell we are going to create the MongoDB collection that will store our chunks and the related vectors. The collection will be created in the `rag_demo` database and will be named as your username.\n",
        "\n"
      ],
      "metadata": {
        "id": "krZWXWaW8avd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo import MongoClient\n",
        "import os\n",
        "\n",
        "mongo_db_name = 'rag_demo'\n",
        "mongo_coll_name = username_widget.value\n",
        "\n",
        "mongo_client = MongoClient(mongodb_uri_widget.value)\n",
        "mongo_coll = mongo_client[mongo_db_name][mongo_coll_name]\n",
        "mongo_db_and_coll_path = '{}.{}'.format(mongo_db_name, mongo_coll_name)\n",
        "\n",
        "# Delete existing documents -- run before demo\n",
        "mongo_coll.delete_many({})\n",
        "\n",
        "doc_count = mongo_coll.count_documents({})\n",
        "'{} document count is {:,}'.format(mongo_db_and_coll_path, doc_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_8wU1VzwpduO",
        "outputId": "e08ff553-8629-4137-a11f-4fd21b0cdd5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rag_demo.picellopaolo document count is 0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 2 - Connect GDrive\n",
        "Allow access to Google Drive"
      ],
      "metadata": {
        "id": "1Wg9Kj-s1MAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mount GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6F5tgUu1O4e",
        "outputId": "cb00e341-a8d3-4aab-a60c-c5350c511abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a folder within your Google Drive account. This folder will be used to upload the documents.\n"
      ],
      "metadata": {
        "id": "qW_6ZzoEE0Lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path\n",
        "from os import path\n",
        "\n",
        "# Just needed in case you'd like to append it to an array\n",
        "data = []\n",
        "folder_name = 'RAG_workshop_documents'\n",
        "folder_path = '/content/drive/MyDrive/' + folder_name\n",
        "\n",
        "if path.exists(folder_path) == False:\n",
        "  os.mkdir(folder_path)"
      ],
      "metadata": {
        "id": "ap3lJ4ql1yb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 3 - Upload some PDFs in the newly created GDrive\n",
        "Upload some sample PDFs in the newly created folder in your Google Drive account.\n",
        "\n",
        "You should find a new folder called ```RAG_workshop_documents``` at the root level of your Google Drive account. You are now ready to drop some PDF documents in this folder.\n",
        "\n",
        "\n",
        "> ‚õî **Do not upload any document containing private, confidential or sensitive data!!** üö´\n",
        "\n",
        "\n",
        "> If you don't know what documents to upload try with the *Practical MongoDB Aggregations Book.pdf* available [here](https://www.practical-mongodb-aggregations.com/)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*Example:*\n",
        "![](https://drive.google.com/uc?export=view&id=1dcEVyvsP3do5t-g_KonJROTUvapUg2aT)\n"
      ],
      "metadata": {
        "id": "cxHew5ASEacw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List documents in the folder\n",
        "for filename in os.listdir(folder_path):\n",
        "      print(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8Ux_mtX7H63",
        "outputId": "0e102793-f5f0-4983-980e-d927aa9d115f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real-Time-Data-via-Event-Driven-Architecture.pdf\n",
            "MongoDB_Atlas_Search_Transforming.pdf\n",
            "MongoDB_Atlas_Security_Controls-v7k3rbhi3p.pdf\n",
            "MongoDB_Best_Practices_Guide.pdf\n",
            "Embedding-GenAI-with-MongoDB.pdf\n",
            "Event-Driven-Applications.pdf\n",
            "MongoDB_&_FSI_in_the_Blockchain_Era.pdf\n",
            "Practical MongoDB Aggregations Book.pdf\n",
            "rag.png\n",
            "sample_documents.png\n",
            "logos.png\n",
            "example_collection.png\n",
            "example_ui.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 4 - Loop thorugh the different files and split them into chunks (by page)"
      ],
      "metadata": {
        "id": "NIY-fEiyHfcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To split document into chunk we leverage Langchain. Each document is splitted into multiple chunks (by page). Each chunk contains the page content and metadata (original document and page number)."
      ],
      "metadata": {
        "id": "jEKogsOHJA9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "chunked_docs = {}\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "      if filename.endswith('pdf'):\n",
        "        print(filename)\n",
        "        loader = PyPDFLoader(os.path.join(folder_path, filename))\n",
        "        chunked_docs[filename] = loader.load_and_split()\n",
        "        print('computed ' + str(len(chunked_docs[filename])) + ' chunks for document: ' + filename)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG4SiM7T9ssc",
        "outputId": "a61a53a0-6c93-462b-ae12-35e77be50976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Real-Time-Data-via-Event-Driven-Architecture.pdf\n",
            "computed 23 chunks for document: Real-Time-Data-via-Event-Driven-Architecture.pdf\n",
            "MongoDB_Atlas_Search_Transforming.pdf\n",
            "computed 23 chunks for document: MongoDB_Atlas_Search_Transforming.pdf\n",
            "MongoDB_Atlas_Security_Controls-v7k3rbhi3p.pdf\n",
            "computed 34 chunks for document: MongoDB_Atlas_Security_Controls-v7k3rbhi3p.pdf\n",
            "MongoDB_Best_Practices_Guide.pdf\n",
            "computed 30 chunks for document: MongoDB_Best_Practices_Guide.pdf\n",
            "Embedding-GenAI-with-MongoDB.pdf\n",
            "computed 17 chunks for document: Embedding-GenAI-with-MongoDB.pdf\n",
            "Event-Driven-Applications.pdf\n",
            "computed 18 chunks for document: Event-Driven-Applications.pdf\n",
            "MongoDB_&_FSI_in_the_Blockchain_Era.pdf\n",
            "computed 20 chunks for document: MongoDB_&_FSI_in_the_Blockchain_Era.pdf\n",
            "Practical MongoDB Aggregations Book.pdf\n",
            "computed 280 chunks for document: Practical MongoDB Aggregations Book.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 5 - Generate Vectors for each chunk\n",
        "We will use the `text-embedding-ada-002` model from OpenAI to generate the vectors. The vector will be stored in the `rag_demo` database"
      ],
      "metadata": {
        "id": "r_-WT2iCtRP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import MongoDBAtlasVectorSearch\n",
        "\n",
        "embeddings_model = OpenAIEmbeddings(\n",
        "    model='text-embedding-ada-002',\n",
        "    openai_api_key=openai_api_key_widget.value\n",
        ")\n",
        "\n",
        "\n",
        "for key,value in chunked_docs.items():\n",
        "  print('Computing vectors for document: ' + key)\n",
        "  vector_db = MongoDBAtlasVectorSearch.from_documents(\n",
        "    value,\n",
        "    embeddings_model,\n",
        "    collection=mongo_coll\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huWedIfotcXO",
        "outputId": "d195482f-1c42-4f06-96d5-f25511343f4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing vectors for document: Real-Time-Data-via-Event-Driven-Architecture.pdf\n",
            "Computing vectors for document: MongoDB_Atlas_Search_Transforming.pdf\n",
            "Computing vectors for document: MongoDB_Atlas_Security_Controls-v7k3rbhi3p.pdf\n",
            "Computing vectors for document: MongoDB_Best_Practices_Guide.pdf\n",
            "Computing vectors for document: Embedding-GenAI-with-MongoDB.pdf\n",
            "Computing vectors for document: Event-Driven-Applications.pdf\n",
            "Computing vectors for document: MongoDB_&_FSI_in_the_Blockchain_Era.pdf\n",
            "Computing vectors for document: Practical MongoDB Aggregations Book.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_count = mongo_coll.count_documents({})\n",
        "'MongoDB document count in {} is {:,}'.format(mongo_db_and_coll_path, doc_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "V5M7gOfst9sc",
        "outputId": "99411f20-b8d5-4a17-c304-5385ec3bfe5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'MongoDB document count in rag_demo.picellopaolo is 445'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The collection in MongoDB will look something like this. As you can see for every MongoDB document we have the reference to the source document (in the `source` field), the content of the page (in the `text` field), the vector (in the `embedding` filed) and the page number (in the `page` field)\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=18qnK-o3MXMH1YZb76UsVLK6IxZ0suY51)"
      ],
      "metadata": {
        "id": "EW1xudsvHJQJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 6 - Create MongoDB Atlas Vector Search index\n",
        "\n",
        "Now that we computed our vectors and loaded our chunks into MongoDB Atlas we need to define a Vector Search Index"
      ],
      "metadata": {
        "id": "LbdBOw-cuzUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo.errors import OperationFailure\n",
        "import inspect\n",
        "\n",
        "mongo_index_def = {\n",
        "    'name': 'rag_demo_index',\n",
        "    'definition': {\n",
        "        'mappings': {\n",
        "            'dynamic': True,\n",
        "            'fields': {\n",
        "                'embedding': {\n",
        "                    'type': 'knnVector',\n",
        "                    'dimensions': 1536,\n",
        "                    'similarity': 'cosine'\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "try:\n",
        "    mongo_coll.create_search_index(mongo_index_def)\n",
        "    print('Search index is building')\n",
        "except OperationFailure as e:\n",
        "    print(e.details['codeName'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qazREgBu5Tu",
        "outputId": "393ccec3-8b34-4404-d554-535dc2df92f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IndexAlreadyExists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 7 - Setup question function\n",
        "\n",
        "We are now ready to test our RAG. Let's first build the function that will be used to submit questions to our LLM. We also retrieve the document sent to the LLM in order to be able to show in the UI what is the document that is sent to the LLM to generate the final answer.\n",
        "\n",
        "In this case the prompt is hidden by Langchain, you can think of it as something like \"*Answer the following question, but only using these documents that I'm passing to you*\")"
      ],
      "metadata": {
        "id": "bchiqivRvNDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "vector_db = MongoDBAtlasVectorSearch.from_connection_string(\n",
        "    mongodb_uri_widget.value,\n",
        "    mongo_db_and_coll_path,\n",
        "    embeddings_model,\n",
        "    index_name='rag_demo_index'\n",
        ")\n",
        "\n",
        "def query_data(query):\n",
        "\n",
        "    # Convert question to vector using OpenAI embeddings\n",
        "    # Perform Atlas Vector Search using Langchain's vectorStore\n",
        "    # similarity_search returns MongoDB documents most similar to the query\n",
        "\n",
        "    docs = vector_db.similarity_search(query, K=1)\n",
        "    as_output = docs[0].page_content\n",
        "\n",
        "    # Leveraging Atlas Vector Search paired with Langchain's QARetriever\n",
        "\n",
        "    # Define the LLM that we want to use -- note that this is the Language Generation Model and NOT an Embedding Model\n",
        "    # If it's not specified (for example like in the code below),\n",
        "    # then the default OpenAI model used in LangChain is OpenAI GPT-3.5-turbo, as of August 30, 2023\n",
        "\n",
        "    llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key_widget.value, temperature=0)\n",
        "\n",
        "    # Get VectorStoreRetriever: Specifically, Retriever for MongoDB VectorStore.\n",
        "    retriever = vector_db.as_retriever()\n",
        "\n",
        "    # Load \"stuff\" documents chain. Stuff documents chain takes a list of documents,\n",
        "    # inserts them all into a prompt and passes that prompt to an LLM.\n",
        "\n",
        "    # Deafault prompt: \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
        "\n",
        "    qa = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=retriever)\n",
        "\n",
        "    # Execute the chain\n",
        "    retriever_output = qa.run(query)\n",
        "\n",
        "    # Return Atlas Vector Search output, and output generated using RAG Architecture\n",
        "    return as_output, retriever_output"
      ],
      "metadata": {
        "id": "YRDv1ab2vQai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 8 - Create UI\n",
        "\n",
        "Let's build a very simple UI to test our RAG"
      ],
      "metadata": {
        "id": "64wN2dULvwvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from gradio.themes.base import Base\n",
        "with gr.Blocks(theme=Base(), title=\"Question Answering App using Vector Search + RAG\") as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # Question Answering App using Atlas Vector Search + RAG Architecture\n",
        "        \"\"\")\n",
        "    textbox = gr.Textbox(label=\"Enter your Question:\")\n",
        "    with gr.Row():\n",
        "        button = gr.Button(\"Submit\", variant=\"primary\")\n",
        "    with gr.Column():\n",
        "        output1 = gr.Textbox(lines=1, max_lines=10, label=\"Output with just Atlas Vector Search (returns text field as is):\")\n",
        "        output2 = gr.Textbox(lines=1, max_lines=10, label=\"Output generated by chaining Atlas Vector Search to Langchain's RetrieverQA + OpenAI LLM:\")\n",
        "\n",
        "# Call query_data function upon clicking the Submit button\n",
        "\n",
        "    button.click(query_data, textbox, outputs=[output1, output2])\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "dW0KoHhlvzPs",
        "outputId": "30cf4186-9ff4-4df7-da7a-56521ee8a4a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://50adcc3b1bc8ce6a26.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://50adcc3b1bc8ce6a26.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example if I ask:\n",
        "\n",
        "> *how can I group documents in mongodb? Can you give me an example?*\n",
        "\n",
        "I can see the relevant chunk retrieved and sent to the LLM for the final answer geenration:\n",
        "\n",
        "\n",
        "\n",
        "> *To group documents in MongoDB, you can use the $group stage in the aggregation pipeline. For example, if you have a collection of flight seats and you want to group them by flight ID and count the number of first class seats, you can use the following aggregation pipeline:*\n",
        "\n",
        "```\n",
        "db.seats.aggregate([\n",
        " {\n",
        "   $match: { \"class\": \"first\" }\n",
        " },\n",
        " {\n",
        "   $group: { _id: \"$flightId\", firstClassSeats: { $sum: 1 } }\n",
        " }\n",
        "])\n",
        "```\n",
        "\n",
        "\n",
        "Feel free to try out with different queries and different documents.\n",
        "\n",
        "Example UI:\n",
        "![](https://drive.google.com/uc?export=view&id=1d-bCZsuH4g-TpOrPVK650ZwflJ3r_OJw)"
      ],
      "metadata": {
        "id": "SfyOo9rpJb-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 9 (optional) - Make it fun: Modify the prompt"
      ],
      "metadata": {
        "id": "v11W-32oQ_I1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.prompt_selector import ConditionalPromptSelector, is_chat_model\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "vector_db = MongoDBAtlasVectorSearch.from_connection_string(\n",
        "    mongodb_uri_widget.value,\n",
        "    mongo_db_and_coll_path,\n",
        "    embeddings_model,\n",
        "    index_name='rag_demo_index'\n",
        ")\n",
        "\n",
        "def query_data_custom_prompt(query):\n",
        "\n",
        "    # Convert question to vector using OpenAI embeddings\n",
        "    # Perform Atlas Vector Search using Langchain's vectorStore\n",
        "    # similarity_search returns MongoDB documents most similar to the query\n",
        "\n",
        "    docs = vector_db.similarity_search(query, K=1)\n",
        "    as_output = docs[0].page_content\n",
        "\n",
        "    # Leveraging Atlas Vector Search paired with Langchain's QARetriever\n",
        "\n",
        "    # Define the LLM that we want to use -- note that this is the Language Generation Model and NOT an Embedding Model\n",
        "    # If it's not specified (for example like in the code below),\n",
        "    # then the default OpenAI model used in LangChain is OpenAI GPT-3.5-turbo, as of August 30, 2023\n",
        "\n",
        "    llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\", openai_api_key=openai_api_key_widget.value, temperature=0)\n",
        "\n",
        "    # Get VectorStoreRetriever: Specifically, Retriever for MongoDB VectorStore.\n",
        "    retriever = vector_db.as_retriever()\n",
        "\n",
        "    ################# MODIFY THE PROMPT ##################\n",
        "    # To keep the code working DO NOT delete the {context} and\n",
        "    # {question} bit of the prompt. You can add instructions\n",
        "    # for the LLM at the beginning or at the end of the prompt\n",
        "    ######################################################\n",
        "\n",
        "    prompt_template = \"\"\"\n",
        "\n",
        "      ***WRITE YOUR MODIFIED PROMPT HERE*** example: I want you to act as a rapper.  You have to give the answer as it was an american rap song, everything should be in ryhme.\n",
        "\n",
        "      Use the following pieces of context to answer the question at the end.If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "\n",
        "      {context}\n",
        "\n",
        "      Question: {question}\n",
        "\n",
        "      Helpful Answer:\"\"\"\n",
        "\n",
        "    PROMPT = PromptTemplate(\n",
        "        template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "\n",
        "    chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "\n",
        "    # Load \"stuff\" documents chain. Stuff documents chain takes a list of documents,\n",
        "    # inserts them all into a prompt and passes that prompt to an LLM.\n",
        "    # Deafault prompt: \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\"\n",
        "    qa = RetrievalQA.from_chain_type(llm, chain_type=\"stuff\", retriever=retriever, chain_type_kwargs=chain_type_kwargs)\n",
        "\n",
        "    # Execute the chain\n",
        "    retriever_output = qa.run(query)\n",
        "\n",
        "    # Return Atlas Vector Search output, and output generated using RAG Architecture\n",
        "    return as_output, retriever_output"
      ],
      "metadata": {
        "id": "B2dVmrOkPK97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_data_custom_prompt(\"what are some best practices when it comes to indexing in mongoDB?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nozr2RtM4nyb",
        "outputId": "078cc90b-ef0a-403f-d1f7-1c4403b40213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('20\\nIndexing\\nEliminate unnecessary indexes. Indexes are resource-intensive and consume RAM and disk. As fields \\nare updated, associated indexes must be maintained, incurring additional CPU and disk I/O overhead. \\nIf you‚Äôre running fully managed databases on MongoDB Atlas, the built-in Performance Advisor \\nsuggests dropping unused, redundant, and hidden indexes to improve write performance and increase \\nstorage space.\\nReduce the size and performance overhead of indexes by only including documents that will be \\naccessed through the index. For example, create a partial index on the orderID field that only includes \\norder documents with an orderStatus of ‚ÄúIn progress, ‚Äù or only indexes the emailAddress field for \\ndocuments where it exists.Figure 7: Remove indexes that are unused, either because the field is not used to query the database or because the \\nindex is redundant.',\n",
              " \" Yo, listen up, I'm here to drop some knowledge\\n      Indexes are important, but don't go overboard, you gotta acknowledge\\n      They take up resources, RAM and disk, so don't be a fool\\n      Keep 'em lean and mean, don't let them become a tool\\n      Use the Performance Advisor, it'll tell you what to do\\n      Drop unused, redundant, and hidden indexes, that's the clue\\n      Only include what you need, don't be wasteful\\n      Partial indexes, my friend, they can be quite tasteful\\n      And when it comes to search, don't rely on the database\\n      It's limited in functionality, it's just not the right package\\n      Use a separate system, it may seem like a chore\\n      But it'll save you headaches, trust me, I've been there before\\n      And don't forget about performance, it's a crucial factor\\n      Too many indexes, and your database will be a disaster\\n      So keep it simple, keep it clean, and you'll be just fine\\n      That's all for now, but remember, these are just guidelines.\")"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some example of outputs using the \"rapper prompt\":\n",
        "\n",
        "\n",
        "```\n",
        "Q: How can i group data in mongodb?\n",
        "\n",
        "MongoDB's Aggregation Framework is the key,\n",
        "It allows you to group data with just a few clicks.\n",
        "An aggregation pipeline is what you need,\n",
        "To process, transform, and return results with speed.\n",
        "\n",
        "The pipeline is a set of operations in order,\n",
        "Each stage takes the output of the previous, no need to border.\n",
        "With $match and $group, you can filter and combine,\n",
        "Reducing code and data manipulation, now that's just fine.\n",
        "\n",
        "Data modeling is important, don't forget,\n",
        "Store related data together, it's a safe bet.\n",
        "Subdocuments and embedded models, oh my,\n",
        "Update related data with just one atomic write, oh my my.\n",
        "\n",
        "So if you want to group data in MongoDB,\n",
        "Just use the Aggregation Framework, it's easy as can be.\n",
        "No need for joins, just store data in a document,\n",
        "With MongoDB, your data will always be on point.\n",
        "```\n",
        "\n",
        "\n",
        "```\n",
        "Q: what are some of the best practices when it comes to security in MongoDB?\n",
        "\n",
        "When it comes to security, MongoDB's got your back\n",
        "Their product security teams work together, that's a fact\n",
        "They find and prevent issues, building features anew\n",
        "Reviewing code and tracking, they know what to do\n",
        "\n",
        "Their customer-facing software goes through a CI/CD\n",
        "With peer-review and automated testing, it's secure as can be\n",
        "And if a security incident does arise\n",
        "MongoDB's got a policy, they'll inform you in no time\n",
        "\n",
        "Patching and change management, they've got it down\n",
        "With automated tools and monitoring, they'll never frown\n",
        "They keep their server software updated, that's for sure\n",
        "And with change management, they've got a QA test plan in store\n",
        "\n",
        "So when it comes to security, MongoDB's got the best\n",
        "With their best practices guide, you'll never be stressed\n",
        "Just follow their tips and you'll be good to go\n",
        "MongoDB's got your back, that's all you need to know.\n",
        "```\n",
        "\n",
        "Feel free to modify the prompt and play around with it to see how it can affect your results.\n"
      ],
      "metadata": {
        "id": "DcFM6Tqg17vC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's update our UI to add a button to call this new function. This will allow us to quickly compare how prompt engineering affect the results of an LLM."
      ],
      "metadata": {
        "id": "sBrkAvTE-80c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks(theme=Base(), title=\"Question Answering App using Vector Search + RAG\") as demo:\n",
        "    gr.Markdown(\n",
        "        \"\"\"\n",
        "        # Question Answering App using Atlas Vector Search + RAG Architecture\n",
        "        \"\"\")\n",
        "    textbox = gr.Textbox(label=\"Enter your Question:\")\n",
        "    with gr.Row():\n",
        "        button = gr.Button(\"standard prompt\", variant=\"primary\")\n",
        "        button2 = gr.Button(\"modifed prompt\", variant=\"primary\")\n",
        "    with gr.Column():\n",
        "        output1 = gr.Textbox(lines=1, max_lines=10, label=\"Output with just Atlas Vector Search (returns text field as is):\")\n",
        "        output2 = gr.Textbox(lines=1, max_lines=10, label=\"Output generated by chaining Atlas Vector Search to Langchain's RetrieverQA + OpenAI LLM:\")\n",
        "\n",
        "# Call query_data function upon clicking the Submit button\n",
        "\n",
        "    button.click(query_data, textbox, outputs=[output1, output2])\n",
        "    button2.click(query_data_custom_prompt, textbox, outputs=[output1, output2])\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "114IO0Wa3rgz",
        "outputId": "fa94fa4a-e59b-4e12-832d-5785efb6b675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://4349b40ec383c936b7.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://4349b40ec383c936b7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example question:\n",
        "\n",
        "```\n",
        "What are some best practices when it comes to indexing in mongoDB?\"\n",
        "```\n",
        "\n",
        "Standard response:\n",
        "```\n",
        "Some best practices for indexing in MongoDB include eliminating unnecessary indexes,\n",
        "reducing the size and performance overhead of indexes, and only including documents\n",
        "that will be accessed through the index. It is also important to consider the limitations\n",
        "and performance overhead of using database search for more sophisticated search experiences.\n",
        "```\n",
        "\n",
        "Modified prompt (rap song) response:\n",
        "```\n",
        "Listen up, I'll give you some tips\n",
        "For indexing in MongoDB, don't let it slip\n",
        "Indexes are resource-intensive, that's a fact\n",
        "So eliminate unnecessary ones, that's the hack\n",
        "\n",
        "As fields are updated, indexes must be maintained\n",
        "That means more CPU and disk I/O, it's a pain\n",
        "But don't worry, I'll tell you what to do\n",
        "To improve write performance, here's a clue\n",
        "\n",
        "If you're on MongoDB Atlas, you're in luck\n",
        "The Performance Advisor will help you unstuck\n",
        "It suggests dropping unused and redundant ones\n",
        "To increase storage space and have more fun\n",
        "\n",
        "Only include documents that will be accessed\n",
        "Through the index, don't leave them neglected\n",
        "Create partial indexes, that's the key\n",
        "For example, only include \"In progress\" orders, you see\n",
        "\n",
        "Or just index the email addresses that exist\n",
        "Don't waste resources, that's the gist\n",
        "So remember, when it comes to indexing\n",
        "Be smart and efficient, that's the best thing.\n",
        "```"
      ],
      "metadata": {
        "id": "Hj7FRld5-Nil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "In this lab we learned how to build a RAG system using MongoDB, OpenAI and LangChain and we saw how powerful prompt engineering can be. This wanted to be just a fun experiment to get you hands-on on this new cool architectural pattern.\n",
        "\n",
        "Now think of your customers and try to uncover use cases where RAG and vector search can bring value.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1yDtpIF9-3IE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Advanced use cases\n",
        "\n",
        "Some possible ways to improve this RAG system:\n",
        "- different chunking techniques\n",
        "- leverage metadata when filtering results (`filter` option in `$vectorSearch`)\n",
        "- prompt engineering (use different prompts)\n",
        "- support multiple data types (txt, gdocs, etc)\n",
        "- combine with keyword search for hybrid search\n"
      ],
      "metadata": {
        "id": "1HS1JYOdwv72"
      }
    }
  ]
}